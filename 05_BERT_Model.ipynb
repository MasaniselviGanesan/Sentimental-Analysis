{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+xR8is5YcfswAvva63iwR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasaniselviGanesan/Sentimental-Analysis/blob/main/05_BERT_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ STEP 1: Install Dependencies\n",
        "!pip install transformers scikit-learn -q\n",
        "\n",
        "# ✅ STEP 2: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "# ✅ STEP 3: Import custom text cleaner\n",
        "sys.path.append('/content/drive/MyDrive/Sentimental-Analysis')\n",
        "from shared_preprocessing import clean_text\n",
        "\n",
        "# ✅ STEP 4: Load and Clean Dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Sentimental-Analysis/data/Tweets.csv\")\n",
        "df = df[['airline_sentiment', 'text']].dropna()\n",
        "df = df[df['airline_sentiment'].isin(['positive', 'neutral', 'negative'])]\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# ✅ STEP 5: Encode Labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['airline_sentiment'])\n",
        "\n",
        "# Save label encoder\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# ✅ STEP 6: Train-Test Split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['clean_text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
        ")\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "# ✅ STEP 7: Load Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# ✅ Optional: Save Tokenizer\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# ✅ STEP 8: Tokenization\n",
        "def tokenize(texts, tokenizer, max_len=128):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize(train_texts, tokenizer)\n",
        "val_encodings = tokenize(val_texts, tokenizer)\n",
        "\n",
        "# ✅ STEP 9: Create Datasets (classic Keras format)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_encodings['input_ids'],\n",
        "        'attention_mask': train_encodings['attention_mask']\n",
        "    },\n",
        "    train_labels\n",
        ")).shuffle(1000).batch(8)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': val_encodings['input_ids'],\n",
        "        'attention_mask': val_encodings['attention_mask']\n",
        "    },\n",
        "    val_labels\n",
        ")).batch(8)\n",
        "\n",
        "# ✅ STEP 10: Load Pretrained Model\n",
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(label_encoder.classes_)\n",
        ")\n",
        "\n",
        "# ✅ STEP 11: Optimizer and Loss (no compute_loss)\n",
        "steps_per_epoch = len(train_dataset) * 5\n",
        "optimizer, _ = create_optimizer(\n",
        "    init_lr=5e-5,\n",
        "    num_train_steps=steps_per_epoch,\n",
        "    num_warmup_steps=0\n",
        ")\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ✅ STEP 12: Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "# ✅ STEP 13: Evaluate\n",
        "pred_logits = model.predict(val_dataset).logits\n",
        "pred_labels = np.argmax(pred_logits, axis=1)\n",
        "print(classification_report(val_labels, pred_labels, target_names=label_encoder.classes_))\n",
        "\n",
        "# ✅ STEP 14: Save to Drive\n",
        "model.save_pretrained(\"/content/drive/MyDrive/tf_bert_sentiment_model/\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/tf_bert_sentiment_model/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSObL-qOb92c",
        "outputId": "bae65b7d-9af3-4a4d-a5fd-a976649266ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1464/1464 [==============================] - 216s 116ms/step - loss: 0.5138 - accuracy: 0.7991 - val_loss: 0.4621 - val_accuracy: 0.8265\n",
            "Epoch 2/5\n",
            "1464/1464 [==============================] - 165s 113ms/step - loss: 0.2932 - accuracy: 0.8932 - val_loss: 0.5251 - val_accuracy: 0.8275\n",
            "Epoch 3/5\n",
            "1464/1464 [==============================] - 164s 112ms/step - loss: 0.1424 - accuracy: 0.9508 - val_loss: 0.6087 - val_accuracy: 0.8299\n",
            "Epoch 4/5\n",
            "1464/1464 [==============================] - 171s 117ms/step - loss: 0.0681 - accuracy: 0.9795 - val_loss: 0.7177 - val_accuracy: 0.8224\n",
            "Epoch 5/5\n",
            "1464/1464 [==============================] - 173s 118ms/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.8057 - val_accuracy: 0.8299\n",
            "366/366 [==============================] - 17s 39ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.90      0.90      1835\n",
            "     neutral       0.66      0.70      0.68       620\n",
            "    positive       0.78      0.74      0.76       473\n",
            "\n",
            "    accuracy                           0.83      2928\n",
            "   macro avg       0.78      0.78      0.78      2928\n",
            "weighted avg       0.83      0.83      0.83      2928\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/tf_bert_sentiment_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/tf_bert_sentiment_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/tf_bert_sentiment_model/vocab.txt',\n",
              " '/content/drive/MyDrive/tf_bert_sentiment_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}